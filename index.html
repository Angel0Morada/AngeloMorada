<!DOCTYPE HTML>
<!--
	Dimension by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>ITE4 Eportfolio</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<div class="logo">
							<span class="icon fa-gem"></span>
						</div>
						<div class="content">
							<div class="inner">
								<h1>Welcome to my Portfolio</h1>
								<p>An ITC C508 Requirement designed by <a>Angelo Hipolito B. Morada</a> and released<br />
								under the <a href="https://html5up.net/license">Creative Commons</a> license.</p>
							</div>
						</div>
						<nav>
							<ul>
								<li><a href="#Me">About Me</a></li>
								<li><a href="#Expectation">Expectation</a></li>
								<li><a href="#schoolworks">School Works</a></li>
								<li><a href="#contact">Contact</a></li>
								<!--<li><a href="#elements">Elements</a></li>-->
							</ul>
						</nav>
					</header>

				<!-- Main -->
					<div id="main">

						<!-- Intro -->
							<article id="Me">
								<h2 class="major">About Me</h2>
								<img src="images/Profile.jpg" alt="Machine Learning" style="width: 50%; max-width: 400px; display: block; margin: 0 auto;" />
								<p style="text-align: justify;">I am Angelo Hipolito B. Morada, currently pursuing a Bachelor of Science in Information Technology at José Rizal University. My interests include playing video games, reading Japanese literature such as manga and light novels, and practicing kendo, a modern Japanese martial art.</p>
							</article>

						<!-- Expectation -->
							<article id="Expectation">
								<h2 class="major">Expectation</h2>
								<img src="images/Expect.jpg" alt="Machine Learning" style="width: 50%; max-width: 400px; display: block; margin: 0 auto;" />
								<p style="text-align: justify;">As a student of ITC-C508, I expect to gain a strong understanding of how advanced machine learning models are developed to perform complex tasks autonomously. The initial readings introduced me to how businesses today are leveraging Machine Learning (ML), Deep Learning (DL), 
									and Natural Language Processing (NLP) to automate operations, improve productivity, and enhance decision-making. I am especially excited to learn how advanced neural networks such as feedforward and recurrent models are trained and optimized to perform predictions or language-related tasks using large volumes of data. 
									For example, real-world tools like Salesforce’s Einstein for sales optimization and DigitalGenius for customer service demonstrate how these technologies are already transforming industries. I hope to acquire practical skills in dataset preparation, model training, and deployment in realistic environments. As AI becomes increasingly integrated into mainstream technology, 
									understanding how these models operate and can be embedded into applications will be a major advantage in my career. I believe the instructor will guide us with valuable insights into advanced ML topics, allowing me to confidently apply them to real-world problems. Ultimately, I want to learn how to design intelligent systems that enhance functionality and drive innovation in the IT industry. 
									With AI playing a growing role across all sectors, this subject is crucial in preparing me to compete and contribute meaningfully in the professional world.
								</p>
								
								
								<h3>References</h3>
									<ul>
									<li>Zhong, Z., & Zhuang, X. (2018). <i>Deep Learning Applications in Business Activities</i>. American Journal of Management Science and Engineering.</li>
									<li>Sriram, V. P., et al. (2021). <i>Role of Machine Learning and Their Effect on Business Management in the World Today</i>. Vidyabharati International Research Journal, IVCIMS.</li>
								</ul>
							</article>

						<!-- School Work -->
							<article id="schoolworks">
								<h2 class="major">Terms</h2>
								
									<nav class="terms-nav">
										<ul class="actions">
											<li><a href="#prelim">Prelim</a></li>
											<li><a href="#midterm">Midterms</a></li>
											<li><a href="#final">Finals</a></li>
										</ul>
									</nav>
							</article>
						<!-- Terms -->
						<article id="prelim">
							<h2 class="major">Prelim</h2>
							    <ul>
									<h2><strong><center>Topics</center></strong></h2>
									<p style="text-align: justify;">
										For the first topic we were refreshed on the topics of ITC C508 we talked about the basics when it comes to machine learning and what is Deep learning. 
										In this first topic Dr. Rodolfo Raga explained how AI works and what are its effects in the industry of IT, then we proceed what is machine learning and its types through the type we then proceeded 
										on the training process of machine learning  and the explanation of Deep Learning and its nature.
									</p>

									<p style="text-align: justify;">
										For the second topic we talked NLP Text preprocessing this topic showed how to process text based data set this topic shows the common data collection, overview steps involving NLP and text preprocessing pipeline. 
										Text preprocessing consists of lower casing – Special Character Removal – Stopword Removal – Tokenization this is commonly done in order to clean the dataset and use it for machine learning data with 
										unprocessed data set it can lead to low output for the models so it is important to properly processed the data in order to create usable machine learning the has a good output.
									</p>

									<p style="text-align: justify;">
										The next topic Introduces Natural Language Processing or NLP this topic shows words representation by using NLP we the machine learning can understand each words in the text based dataset just like the second 
										we need to preprocess the data before we implement Text Representation which will help the model understand the words. With text representation it has three text representation techniques that one will count the frequency, 
										while the other understand the weight of the word and the last will find the position of the text in the sentence in the dataset.
									</p>

									<p style="text-align: justify;">
										The third topic explains the application of NLP applications in real world applications. it shows that NLP can be used when it comes to understanding sentence’s meaning and text classification which can be applied on social media 
										monitoring for Sentiment Analysis, chatbots on Question learning, language translation on Machine Translation, and lastly conversion of spoken words to written text for Speech to text. All of this can be done through 
										the application of NLP with the right training and dataset NLP can basically do all the thing mentioned.
									</p>

									<p style="text-align: justify;">
										The last topic for the prelims is named entity recognition or NER we were introduced to spacy a free open source python library that is use for NLP, a development focused library it is designed to process large amount of text volumes 
										to be later used by models. The processing pipeline consist of Tokenizer - Tagger - Parser - NER - … - Doc, with the use of the whole pipeline we can apply depending on the situation can turn a noisy raw dataset text into a structured 
										ready to use data.
									</p>
									<h2><strong><center>Exercises</center></strong></h2>
									<img src="images/web.png" alt="Machine Learning" style="width: 50%; max-width: 400px; display: block; margin: 0 auto;" />
									<p style="text-align: justify;">
										The First exercise was meant to create a E-portfolio to see our progress as the school year progresses. In this exercise I created a website that will hold all the topics, activities, quizzes, and exam that I will per terms, every end 
										of the term I will post the said objects in this website to be checked by our professor. I used a Simple Php web development kit for the website and add few things in the layout and design to make it my own.
									</p>
									<img src="images/exer1.png" alt="Machine Learning" style="width: 50%; max-width: 400px; display: block; margin: 0 auto;" />	
									<p style="text-align: justify;">
										In the second exercise we were tasked to understand the process of preprocessing a text base data in NLP, this gave us a glimpse of working in a realworld user generated data in my instance the data is about game review. We followed a well structured 
										pipeline made by our professor but I add a step where in the data has URL base text so I created an instance to remove the said URLs transforming a noisy and unprocessed data into cleaned and noise-free ready to use data. The activity taught me the 
										importance of preprocessing because with unprocessed data the model will badly perform and create unwanted output, but by processing the data the model can yield a wanted result and perform good results.
									</p>
									<img src="images/exer2.png" alt="Machine Learning" style="width: 50%; max-width: 400px; display: block; margin: 0 auto;" />
									<p style="text-align: justify;">
										In the third exercise we were tasked to understand the different text representation techniques can be applied for the NLP that will then be converted to structured numerical data. In my understanding by applying Bag-of-Words, and 
										Term Frequency–Inverse Document Frequency (TF-IDF) we can set up the data for model use by turning text data into a numerical cleaned formed ready for models to use, the method has a special way of turning them into numerical format each of the methods 
										has a different perception when it comes to trade-offs between simplicity, interpretability, and efficiency. This activity shows me that the three methods used in this activity has their own use case scenario they have their own strengths and weaknesses 
										but thoroughly useful to the different industries that is applicable to them.
									</p>
									<img src="images/exer3.png" alt="Machine Learning" style="width: 50%; max-width: 400px; display: block; margin: 0 auto;" />
									<p style="text-align: justify;">
										For the last activity of the preliminary period we were tasked to understand the importance of NER model in a organized and carful way. This evaluation how the model functioned well in an entity commonality such as place but perform inconsistent when it 
										comes to categories like numbers. From what I learned the activity showed that the behavior of the model varies depending on different data and improved its performance by proper tuning and preprocessing.
									</p>
									<h2><strong><center>Exams</center></strong></h2>
									<p style="text-align: justify;">
										The exam consists of three questions, the first question was for our quiz, and the remaining questions are the exam itself. For the first question which is the quiz we were asked what the challenges were on getting the “golden standard data” 
										and the exam question that consist of two questions which is the importance of preprocessing for bag of words and the last question is the new NLP technique learned while we were doing the activities. In the first question I emphasized that 
										language was one of the challenges and the continued preprocess to achieve the golden standard for the dataset in order to achieve the standard the data must be continuously processed to achieve it and if the language is also include the process 
										is mostly doubled since we need to tune it before we continue. As for the first exam question, the gist of what I wrote is that the preprocess is important because an unprocessed data can yield to low performing model but with proper preprocess 
										it can yield good results and high-performance outcomes. And lastly for the last question since the dataset that I used had some URLs I have to add another technique to remove the URLs in the dataset so I create a process to remove them.
									</p>
									<h2><strong><center>Learning Reflection</center></strong></h2>
									<p style="text-align: justify;">
										During this term we were taught the basics of processing text based data and its importance. I should take account the output of model is based on how the data is processed, if I were to used and unprocessed data and feed it to a model the expected 
										output would be and underperforming model with an unfavorable output but with the proper preprocessing and technique application the model will perform great and will yield a favorable output. Also I need to further understand the use of the first 
										model we used this term which is NER because the paper that I wrote seems to be rushed and understudied.
									</p>
								</ul>
							</article>

							<article id="midterm">
							<h2 class="major">Midterms</h2>
							    <ul>
									<h2><strong><center>Topics</center></strong></h2>
									<p style="text-align: justify;">
										In the midterm topics, we discussed NLP models and evaluation metrics, the BERT model, and the fundamentals of LLMs, LangChain, and RAG. These lessons introduced the NLP tasks and model architectures that will be applied in our Final Term project. 
										During the first week of midterms, we learned several NLP tasks that can be used in real-world scenarios. After that, we were introduced to the BERT model, specifically its ability to perform Question-and-Answer (QnA) tasks. 
										Finally, we were assigned to research Large Language Models (LLMs), LangChain, and Retrieval-Augmented Generation (RAG), which became the main coverage of the midterm examination.
									</p>
									<p style="text-align: justify;">
										For the first topic, we studied model analysis, the pipeline used for NLP models, and the proper evaluation metrics required to assess their performance. We were introduced to common NLP analysis tasks such as Sentiment Analysis, and then we proceeded to learn the Model Assessment Pipeline, 
										where we evaluate a model’s performance using either intrinsic (internal evaluation) methods or extrinsic (real-world task) methods. After this, we discussed the different performance metrics that can be used for evaluation, such as accuracy, F-score, precision, and recall.
									</p>
									<p style="text-align: justify;">
										The second topic in the midterm focused on the NLP model BERT, which was used to perform Question-and-Answer (QnA) tasks. We studied the concepts of language understanding and language generation, both of which are essential in QnA systems. Language understanding allows the model to interpret 
										the meaning and sentiment of the question, while language generation enables the model to produce an appropriate answer. In this topic, we also learned the differences between BERT (an encoder-based model) and GPT (a decoder-based model). BERT reads text bidirectionally from left to right and 
										right to left simultaneously and is mainly designed for understanding language. GPT, on the other hand, reads text left to right and is primarily used for generating language.
									</p>
									<p style="text-align: justify;">
										The third topic introduced us to chatbots, which are computer programs designed to create natural and meaningful communication between humans and machines. These systems rely on high-level NLP algorithms to generate responses and are commonly used in entertainment, education, customer service, 
										and many technology-based industries. In this topic, we also studied LangChain, a framework that connects large language models with external tools and data sources to create more powerful and customizable applications. We likewise explored RAG (Retrieval-Augmented Generation), 
										a technique that improves chatbot accuracy by retrieving relevant information from a knowledge source before generating a response. Together, these concepts helped us understand how modern chatbots operate and handle real-world queries.
									</p>
									<h2><strong><center>Exercises</center></strong></h2>
									<img src="images/exerM1.png" alt="Machine Learning" style="width: 50%; max-width: 400px; display: block; margin: 0 auto;" />
									<p style="text-align: justify;">
										For the first exercise in the midterm, we were tasked with documenting in an e-portfolio everything we studied and developed during the preliminary period of the semester. This documentation included our experiences, understanding of the lessons, and reflections on the tasks and topics completed during that term.
									</p>
									<img src="images/exerM2.png" alt="Machine Learning" style="width: 50%; max-width: 400px; display: block; margin: 0 auto;" />
									<p style="text-align: justify;">
										The second exercise involved implementing and evaluating four core NLP tasks using a rental review dataset: Spam/HAM Classification, Sentiment Analysis, Part-of-Speech (POS) Tagging, and Text Summarization. Using spaCy and scikit-learn, each model was trained, tested, and evaluated using appropriate metrics such as accuracy, precision, 
										recall, F1-score, ROUGE, cosine similarity, and compression ratio. The results showed strong performance in the classification tasks, with Spam/HAM achieving 90% accuracy and Sentiment Analysis reaching 92.2%. POS tagging performed reliably using a pre-trained model, while the extractive summarizer produced concise summaries with moderate ROUGE and high cosine similarity scores. 
										Overall, the exercise demonstrated how NLP models can process unstructured text and highlighted the importance of proper evaluation in assessing model effectiveness.
									</p>
									<img src="images/exerM3.png" alt="Machine Learning" style="width: 50%; max-width: 400px; display: block; margin: 0 auto;" />
									<p style="text-align: justify;">
										The third exercise focused on developing and evaluating a BERT-based Question-and-Answer (QnA) system using a Philippine News Agency article as the context. Three pretrained transformer models BERT-base, DistilBERT, and BERT-large were tested on ten manually created questions. The models were evaluated using Exact Match (EM), F1 score, and inference time. 
										Results showed that EM scores were low (20–30%) due to strict word-for-word matching, while F1 scores (59–61%) better reflected the models’ ability to capture correct meanings. 
										DistilBERT delivered the fastest inference time (~3 seconds per query) while maintaining accuracy similar to the larger models. Overall, the exercise demonstrated the trade-off between model size, accuracy, and efficiency, highlighting DistilBERT as the most practical choice for real-time QnA applications.
									</p>
									<img src="images/exerM4.png" alt="Machine Learning" style="width: 50%; max-width: 400px; display: block; margin: 0 auto;" />
									<p style="text-align: justify;">
										The final exercise in the midterm focused on researching Large Language Models (LLMs), LangChain, and Retrieval-Augmented Generation (RAG). The study examined why AI chatbots often produce inaccurate or misleading responses, highlighting issues such as hallucinations, outdated knowledge, and overly confident wrong answers. 
										RAG was introduced as a solution that improves accuracy by retrieving relevant information from external sources before generating a response. LangChain was also analyzed as a tool that helps developers build RAG systems by providing components such as document loaders, text splitters, vector stores, and retrievers. 
										Overall, the exercise emphasized how combining RAG with LangChain leads to more reliable and grounded chatbot responses.
									</p>

									<h2><strong><center>Exams & Quizzes</center></strong></h2>
									<img src="images/MQ1.jpg" alt="Machine Learning" style="width: 50%; max-width: 400px; display: block; margin: 0 auto;" />
									<p style="text-align: justify;">
										In the quiz section, I was asked how LangChain and RAG work together to use external data and help LLMs generate more accurate outputs. I explained that the user begins by creating a question (query), which is sent to the LLM. Then, LangChain organizes the process by connecting the LLM to the needed tools and data. After that, 
										RAG retrieves updated and relevant information from external sources. This retrieved data is then used by the model to generate a more accurate and reliable answer.
									</p>
									<img src="images/MT1.jpg" alt="Machine Learning" style="width: 50%; max-width: 400px; display: block; margin: 0 auto;" />
									<p style="text-align: justify;">
										In the exam, one scenario asked me to explain how the two-fold process works and how it allows BERT to determine the sentiment of a text. I was also asked to describe how BERT’s training process helps it understand word context better than traditional algorithms. 
										To answer briefly, I did not perform well on this part because I did not fully understand how the two-fold process and its effects on BERT’s behavior in its applications.
									</p>
									<h2><strong><center>Learning Reflection</center></strong></h2>
									<p style="text-align: justify;">
										Throughout the midterm, I gained a deeper understanding of how NLP models, QnA systems, and advanced tools like LangChain and RAG work together to improve real-world AI applications. 
										The exercises helped me connect the concepts from the prelims such as NLP tasks and evaluation metrics to more complex models like BERT and DistilBERT.
									</p>
									<p style="text-align: justify;">
										In the BERT QnA exercise, I learned how pretrained models process text, retrieve answers from context, and why metrics like Exact Match and F1 are important for evaluating performance. I also realized the importance of balancing accuracy and speed, 
										as seen in how DistilBERT delivered faster responses with similar accuracy to larger models.
									</p>
									<p style="text-align: justify;">
										The research on LLMs, LangChain, and RAG expanded my understanding of why chatbots sometimes provide inaccurate information and how retrieval-based methods can fix this. 
										I now see how tools like LangChain help organize workflows and how RAG grounds model outputs in reliable data, making them more trustworthy.
									</p>
									<p style="text-align: justify;">
										Overall, the midterm taught me not only how these models work but also why they matter in real-world applications. It also made me aware of areas I still need to improve especially in understanding BERT’s internal processes, such as its two-fold architecture. 
										This reflection process helped me see my progress and identify what I need to study further to perform better in future assessments.
									</p>
								</ul>
							</article>

							<article id="final">
							<h2 class="major">Finals</h2>
							    <ul>
									<h2><strong><center>Topics</center></strong></h2>
									<p style="text-align: justify;">
										In the final term, most of our time was dedicated to developing the final project, but we still covered two important topics. 
										One topic focused on fine-tuning pretrained models, and the other discussed the automated optimization process for BERT fine-tuning.
									</p>
									<p style="text-align: justify;">
										In the first topic, we learned the difference between pre-training and fine-tuning in BERT models. Pre-training helps the model learn general language patterns, grammar, structure, and broad knowledge, while fine-tuning adapts the model to a specific task to improve its performance on that particular application.
									</p>
									<p style="text-align: justify;">
										In the second and last topic, we explored the automation process of fine-tuning a BERT model. This discussion introduced different hyperparameter tuning strategies. Babysitting involves manually adjusting each parameter to test and observe the output. Grid Search systematically tests all possible parameter combinations, but it is slow and requires significant resources. 
										Random Search selects parameter combinations randomly, making it faster and more efficient, especially in situations where time and resources are limited.
									</p>
									<h2><strong><center>Exercises</center></strong></h2>
									<img src="images/exerF1.png" alt="Machine Learning" style="width: 50%; max-width: 400px; display: block; margin: 0 auto;" />
									<p style="text-align: justify;">
										The first exercise in the finals focused on fine-tuning a DistilBERT model for classifying MMDA traffic incident tweets. Using the “Manila Traffic Incident Data” dataset, I followed a complete workflow that included cleaning the text data, tokenizing the tweets, preparing label mappings, and configuring the model for training. Several hyperparameter combinations were tested, 
										and each configuration was evaluated using accuracy, precision, recall, and F1-score. Among the different setups, the configuration with weight_decay = 0.03, warmup_steps = 750, and logging_steps = 50 achieved the best results with 96.58% accuracy and a 95.44% F1-score. This exercise demonstrated how proper hyperparameter tuning greatly affects model performance and showed that transformer models like DistilBERT can effectively classify real-world traffic incident tweets from MMDA.
									</p>
									<img src="images/exerF2.png" alt="Machine Learning" style="width: 50%; max-width: 400px; display: block; margin: 0 auto;" />
									<p style="text-align: justify;">
										The second exercise focused on the automated hyperparameter optimization of a DistilBERT model using Optuna’s Random Search framework. The goal was to improve the performance of a traffic incident classification model trained on the MMDA Traffic Incident dataset. After preprocessing the data, initializing the model, and preparing evaluation metrics, Optuna automatically tested different combinations of learning rate, weight decay, and number of training epochs. 
										Three optimization trials were executed, and each trial showed improvements over the baseline configuration. The best-performing setup achieved an accuracy of 0.967 and an F1-score of 0.957, using a learning rate of 4.0e-05, six training epochs, and a weight decay of 0.01. This exercise demonstrated how automated Random Search reduces manual tuning time while still finding effective hyperparameter combinations for improving DistilBERT’s classification performance on real-world MMDA traffic data.
									</p>
									<img src="images/P1.png" alt="Machine Learning" style="width: 50%; max-width: 400px; display: block; margin: 0 auto;" />
									<p style="text-align: justify;">
										The final project focused on building an AI-driven traffic congestion detection system using a fine-tuned DistilBERT sentiment classifier applied to MMDA traffic-related tweets.
										 The workflow involved preparing the dataset, cleaning traffic text, generating sentiment labels using VADER when needed, and converting the data into a format compatible with Hugging Face. The model was fine-tuned through repeated training trials, and a Random Search procedure was used to identify the best hyperparameter combination for sentiment classification.
									</p>
									<p style="text-align: justify;">
										After optimization, the best-performing model achieved 95.88% accuracy and a 95.79% F1-score, showing stable and consistent results across multiple trials. A real-time inference module was then developed, combining the classifier with keyword detection and rule-based location extraction to determine whether a tweet indicated likely or unlikely traffic congestion. The system successfully recognized stalled vehicles, detected relevant locations, and avoided false congestion alerts in normal-traffic cases.
									</p>
									<p style="text-align: justify;">
										Overall, the project demonstrated that lightweight transformer models, paired with hyperparameter tuning and rule-based heuristics, can support real-time monitoring of urban traffic and contribute to data-driven transportation solutions.
									</p>
									<h2><strong><center>Exams & Quizzes</center></strong></h2>
									<p style="text-align: justify;">
										The exam and quizzes were taken on the same day. In the quiz portion, I was asked about the difference between Random Search and Grid Search. I explained that Grid Search tests every possible hyperparameter combination to find the best-performing setup, while Random Search randomly selects combinations and evaluates which one works best.
									</p>
									<p style="text-align: justify;">
										In the exam portion, we were asked to create a diagram showing pre-training and fine-tuning and then explain the difference between them. I created a diagram that illustrated how pre-training focuses on learning language structure, grammar, and general meaning, while fine-tuning trains the model on a specific task to improve its performance for that particular application.
									</p>
									<p style="text-align: justify;">
										For the final part of the exam, we were asked to describe our contributions to the project, identify one challenge we encountered, and explain how we overcame it. I answered that I developed the entire pipeline, and the challenge I faced was that even small adjustments could cause major effects on the system. I overcame this by carefully testing, debugging, and ensuring each fix did not break the flow of the overall pipeline.
									</p>
									<h2><strong><center>Learning Reflection</center></strong></h2>
									<p style="text-align: justify;">
										In the final term, I learned how to properly fine-tune and optimize transformer models like DistilBERT. Working on the exercises and the final project helped me understand how important preprocessing, hyperparameter tuning, and evaluation metrics are in building an effective NLP model. I also realized how small changes in parameters can greatly affect the model’s performance.
									</p>
									<p style="text-align: justify;">
										Building the full pipeline for the project was challenging, especially because even minor fixes could affect the entire system. However, by testing and debugging carefully, I was able to make everything work. This experience made me more confident in applying NLP techniques to real-world problems, especially in traffic detection and analysis. Overall, the final term helped me improve my technical skills and understand how to create reliable NLP workflows from start to finish.
									</p>
								</ul>
						</article>

						<!-- Contact -->
							<article id="contact">
								<h2 class="major">Contact</h2>
								
								<ul class="icons">
									<li><a href="https://www.facebook.com/alho.o.morada" class="icon brands fa-facebook-f"><span class="label">Facebook</span></a></li>
									<li><a href="https://github.com/Angel0Morada" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
								</ul>
							</article>

						<!-- Elements -->
							<article id="elements">
								<h2 class="major">Elements</h2>

								<section>
									<h3 class="major">Text</h3>
									<p>This is <b>bold</b> and this is <strong>strong</strong>. This is <i>italic</i> and this is <em>emphasized</em>.
									This is <sup>superscript</sup> text and this is <sub>subscript</sub> text.
									This is <u>underlined</u> and this is code: <code>for (;;) { ... }</code>. Finally, <a href="#">this is a link</a>.</p>
									<hr />
									<h2>Heading Level 2</h2>
									<h3>Heading Level 3</h3>
									<h4>Heading Level 4</h4>
									<h5>Heading Level 5</h5>
									<h6>Heading Level 6</h6>
									<hr />
									<h4>Blockquote</h4>
									<blockquote>Fringilla nisl. Donec accumsan interdum nisi, quis tincidunt felis sagittis eget tempus euismod. Vestibulum ante ipsum primis in faucibus vestibulum. Blandit adipiscing eu felis iaculis volutpat ac adipiscing accumsan faucibus. Vestibulum ante ipsum primis in faucibus lorem ipsum dolor sit amet nullam adipiscing eu felis.</blockquote>
									<h4>Preformatted</h4>
									<pre><code>i = 0;

while (!deck.isInOrder()) {
    print 'Iteration ' + i;
    deck.shuffle();
    i++;
}

print 'It took ' + i + ' iterations to sort the deck.';</code></pre>
								</section>

								<section>
									<h3 class="major">Lists</h3>

									<h4>Unordered</h4>
									<ul>
										<li>Dolor pulvinar etiam.</li>
										<li>Sagittis adipiscing.</li>
										<li>Felis enim feugiat.</li>
									</ul>

									<h4>Alternate</h4>
									<ul class="alt">
										<li>Dolor pulvinar etiam.</li>
										<li>Sagittis adipiscing.</li>
										<li>Felis enim feugiat.</li>
									</ul>

									<h4>Ordered</h4>
									<ol>
										<li>Dolor pulvinar etiam.</li>
										<li>Etiam vel felis viverra.</li>
										<li>Felis enim feugiat.</li>
										<li>Dolor pulvinar etiam.</li>
										<li>Etiam vel felis lorem.</li>
										<li>Felis enim et feugiat.</li>
									</ol>
									<h4>Icons</h4>
									<ul class="icons">
										<li><a href="#" class="icon brands fa-facebook-f"><span class="label">Facebook</span></a></li>
										<li><a href="#" class="icon brands fa-github"><span class="label">Github</span></a></li>
									</ul>

									<h4>Actions</h4>
									<ul class="actions">
										<li><a href="#" class="button primary">Default</a></li>
										<li><a href="#" class="button">Default</a></li>
									</ul>
									<ul class="actions stacked">
										<li><a href="#" class="button primary">Default</a></li>
										<li><a href="#" class="button">Default</a></li>
									</ul>
								</section>

								<section>
									<h3 class="major">Table</h3>
									<h4>Default</h4>
									<div class="table-wrapper">
										<table>
											<thead>
												<tr>
													<th>Name</th>
													<th>Description</th>
													<th>Price</th>
												</tr>
											</thead>
											<tbody>
												<tr>
													<td>Item One</td>
													<td>Ante turpis integer aliquet porttitor.</td>
													<td>29.99</td>
												</tr>
												<tr>
													<td>Item Two</td>
													<td>Vis ac commodo adipiscing arcu aliquet.</td>
													<td>19.99</td>
												</tr>
												<tr>
													<td>Item Three</td>
													<td> Morbi faucibus arcu accumsan lorem.</td>
													<td>29.99</td>
												</tr>
												<tr>
													<td>Item Four</td>
													<td>Vitae integer tempus condimentum.</td>
													<td>19.99</td>
												</tr>
												<tr>
													<td>Item Five</td>
													<td>Ante turpis integer aliquet porttitor.</td>
													<td>29.99</td>
												</tr>
											</tbody>
											<tfoot>
												<tr>
													<td colspan="2"></td>
													<td>100.00</td>
												</tr>
											</tfoot>
										</table>
									</div>

									<h4>Alternate</h4>
									<div class="table-wrapper">
										<table class="alt">
											<thead>
												<tr>
													<th>Name</th>
													<th>Description</th>
													<th>Price</th>
												</tr>
											</thead>
											<tbody>
												<tr>
													<td>Item One</td>
													<td>Ante turpis integer aliquet porttitor.</td>
													<td>29.99</td>
												</tr>
												<tr>
													<td>Item Two</td>
													<td>Vis ac commodo adipiscing arcu aliquet.</td>
													<td>19.99</td>
												</tr>
												<tr>
													<td>Item Three</td>
													<td> Morbi faucibus arcu accumsan lorem.</td>
													<td>29.99</td>
												</tr>
												<tr>
													<td>Item Four</td>
													<td>Vitae integer tempus condimentum.</td>
													<td>19.99</td>
												</tr>
												<tr>
													<td>Item Five</td>
													<td>Ante turpis integer aliquet porttitor.</td>
													<td>29.99</td>
												</tr>
											</tbody>
											<tfoot>
												<tr>
													<td colspan="2"></td>
													<td>100.00</td>
												</tr>
											</tfoot>
										</table>
									</div>
								</section>

								<section>
									<h3 class="major">Buttons</h3>
									<ul class="actions">
										<li><a href="#" class="button primary">Primary</a></li>
										<li><a href="#" class="button">Default</a></li>
									</ul>
									<ul class="actions">
										<li><a href="#" class="button">Default</a></li>
										<li><a href="#" class="button small">Small</a></li>
									</ul>
									<ul class="actions">
										<li><a href="#" class="button primary icon solid fa-download">Icon</a></li>
										<li><a href="#" class="button icon solid fa-download">Icon</a></li>
									</ul>
									<ul class="actions">
										<li><span class="button primary disabled">Disabled</span></li>
										<li><span class="button disabled">Disabled</span></li>
									</ul>
								</section>

								<section>
									<h3 class="major">Form</h3>
									<form method="post" action="#">
										<div class="fields">
											<div class="field half">
												<label for="demo-name">Name</label>
												<input type="text" name="demo-name" id="demo-name" value="" placeholder="Jane Doe" />
											</div>
											<div class="field half">
												<label for="demo-email">Email</label>
												<input type="email" name="demo-email" id="demo-email" value="" placeholder="jane@untitled.tld" />
											</div>
											<div class="field">
												<label for="demo-category">Category</label>
												<select name="demo-category" id="demo-category">
													<option value="">-</option>
													<option value="1">Manufacturing</option>
													<option value="1">Shipping</option>
													<option value="1">Administration</option>
													<option value="1">Human Resources</option>
												</select>
											</div>
											<div class="field half">
												<input type="radio" id="demo-priority-low" name="demo-priority" checked>
												<label for="demo-priority-low">Low</label>
											</div>
											<div class="field half">
												<input type="radio" id="demo-priority-high" name="demo-priority">
												<label for="demo-priority-high">High</label>
											</div>
											<div class="field half">
												<input type="checkbox" id="demo-copy" name="demo-copy">
												<label for="demo-copy">Email me a copy</label>
											</div>
											<div class="field half">
												<input type="checkbox" id="demo-human" name="demo-human" checked>
												<label for="demo-human">Not a robot</label>
											</div>
											<div class="field">
												<label for="demo-message">Message</label>
												<textarea name="demo-message" id="demo-message" placeholder="Enter your message" rows="6"></textarea>
											</div>
										</div>
										<ul class="actions">
											<li><input type="submit" value="Send Message" class="primary" /></li>
											<li><input type="reset" value="Reset" /></li>
										</ul>
									</form>
								</section>

							</article>

					</div>

				<!-- Footer -->
					<footer id="footer">
						<p class="copyright">&copy; Malarod: <a href="https://html5up.net">HTML5 UP</a>.</p>
					</footer>

			</div>

		<!-- BG -->
			<div id="bg"></div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
